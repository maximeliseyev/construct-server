// Delivery Worker - Simplified Architecture (Redis Lists)
// ============================================================================
//
// This worker consumes messages from Kafka and delivers them to users via Redis Lists.
//
// –£–ü–†–û–©–Å–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
// ============================================================================
//
// –ü–†–û–ë–õ–ï–ú–ê —Å—Ç–∞—Ä–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:
// - Pub/Sub - fire-and-forget - –µ—Å–ª–∏ construct-server –Ω–µ —Å–ª—É—à–∞–µ—Ç, —Å–æ–æ–±—â–µ–Ω–∏–µ –ü–û–¢–ï–†–Ø–ù–û!
// - –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –æ—Ç–∫–∞–∑–∞: Kafka ‚Üí delivery-worker ‚Üí Pub/Sub ‚Üí construct-server
//
// –†–ï–®–ï–ù–ò–ï:
// - –ò—Å–ø–æ–ª—å–∑—É–µ–º Redis Lists (RPUSH) –≤–º–µ—Å—Ç–æ Pub/Sub
// - Redis Lists persistent - —Å–æ–æ–±—â–µ–Ω–∏—è –ù–ï —Ç–µ—Ä—è—é—Ç—Å—è
// - construct-server –æ–ø—Ä–∞—à–∏–≤–∞–µ—Ç delivery_queue —á–µ—Ä–µ–∑ poll_delivery_queue
//
// Flow:
// 1. Kafka Consumer —á–∏—Ç–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ topic (partitioned by recipient_id)
// 2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è:
//    - Check deduplication (Redis processed_msg:{message_id})
//    - Check recipient online via user:{user_id}:server_instance_id
//    - If online: RPUSH to delivery_queue:{server_instance_id}
//    - If offline: RPUSH to delivery_queue:offline:{recipient_id}
//    - Commit Kafka offset
// 3. construct-server polls delivery_queue –∏ –¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —á–µ—Ä–µ–∑ WebSocket
//
// –ì–∞—Ä–∞–Ω—Ç–∏–∏:
// - At-least-once delivery (Kafka + Redis Lists)
// - –°–æ–æ–±—â–µ–Ω–∏—è –ù–ï —Ç–µ—Ä—è—é—Ç—Å—è (Redis Lists persistent)
// - Deduplication via Redis (processed_msg:{message_id})
// - Ordering per user (Kafka partitioning + Redis FIFO)
// ============================================================================

use anyhow::{Context, Result};
use construct_server::config::Config;
use construct_server::kafka::{KafkaMessageEnvelope, MessageConsumer};
use construct_server::utils::log_safe_id;
use futures_util::stream::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::HashSet;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};

/// Notification that a user has come online
#[derive(Debug, Clone, Serialize, Deserialize)]
struct UserOnlineNotification {
    /// User ID that came online
    user_id: String,

    /// Unique ID of the server instance handling this user's connection
    server_instance_id: String,
}

/// Shared state for Kafka-based delivery worker
struct WorkerState {
    config: Arc<Config>,
    redis_conn: Arc<RwLock<redis::aio::MultiplexedConnection>>,
    redis_client: Arc<redis::Client>,
    /// Counter for consecutive "no delivery queues" errors (for periodic summary logging)
    #[allow(dead_code)]
    no_queues_count: Arc<RwLock<u64>>,
    /// Phase 4: Shadow-read mode enabled
    /// When true, compares Kafka messages with Redis offline queues for validation
    #[allow(dead_code)]
    shadow_read_enabled: bool,
    /// Track message IDs processed from Kafka (for reverse-check in Phase 4.3)
    #[allow(dead_code)]
    processed_message_ids: Arc<RwLock<HashSet<String>>>,
    /// Users who came online (for logging and monitoring)
    users_online_notifications: Arc<RwLock<HashSet<String>>>,
}

#[tokio::main]
async fn main() -> Result<()> {
    // Load configuration
    let config = Config::from_env()?;
    let config = Arc::new(config);

    // Initialize tracing
    tracing_subscriber::registry()
        .with(tracing_subscriber::EnvFilter::new(config.rust_log.clone()))
        .with(tracing_subscriber::fmt::layer())
        .init();

    info!("=== Delivery Worker Starting ===");
    info!("Kafka Enabled: {}", config.kafka.enabled);
    info!("Kafka Brokers: {}", config.kafka.brokers);
    info!("Kafka Topic: {}", config.kafka.topic);
    info!("Kafka Consumer Group: {}", config.kafka.consumer_group);

    // Mask credentials in Redis URL for logging
    let redis_url_safe = if let Some(at_pos) = config.redis_url.find('@') {
        let protocol_end = config.redis_url.find("://").map(|p| p + 3).unwrap_or(0);
        format!(
            "{}***{}",
            &config.redis_url[..protocol_end],
            &config.redis_url[at_pos..]
        )
    } else {
        config.redis_url.clone()
    };
    info!("Connecting to Redis at: {}", redis_url_safe);

    // Connect to Redis
    let client =
        redis::Client::open(config.redis_url.as_str()).context("Failed to create Redis client")?;

    let redis_conn = client
        .get_multiplexed_async_connection()
        .await
        .context("Failed to create Redis data connection")?;

    info!("Connected to Redis");

    let redis_conn = Arc::new(RwLock::new(redis_conn));

    // Phase 4: Shadow-read mode (compare Kafka vs Redis)
    let shadow_read_enabled = config.worker.shadow_read_enabled;

    if shadow_read_enabled {
        info!("üîç Shadow-read mode ENABLED - will compare Kafka vs Redis offline queues");
    }

    let state = Arc::new(WorkerState {
        config: config.clone(),
        redis_conn: redis_conn.clone(),
        redis_client: Arc::new(client),
        no_queues_count: Arc::new(RwLock::new(0)),
        shadow_read_enabled,
        processed_message_ids: Arc::new(RwLock::new(HashSet::new())),
        users_online_notifications: Arc::new(RwLock::new(HashSet::new())),
    });

    // Choose delivery mode based on KAFKA_ENABLED
    if config.kafka.enabled {
        info!("Starting Kafka consumer mode");
        run_kafka_consumer_mode(state).await
    } else {
        error!("Redis-only mode is not supported. Please enable KAFKA_ENABLED=true");
        Err(anyhow::anyhow!("Redis-only mode is deprecated. Use Kafka mode instead."))
    }
}

/// Kafka-based delivery worker (Phase 3+)
async fn run_kafka_consumer_mode(state: Arc<WorkerState>) -> Result<()> {
    // Initialize message consumer
    let consumer =
        MessageConsumer::new(&state.config.kafka).context("Failed to initialize Kafka consumer")?;

    info!("Kafka consumer initialized successfully");
    info!(
        "Polling messages from Kafka topic: {}",
        state.config.kafka.topic
    );

    // Spawn ACK consumer in background (Solution 1D)
    // Note: ACK consumer disabled in Phase 5 Kafka-only mode
    // Delivery ACKs are handled directly by the server
    // TODO: Re-enable if ACK processing via Kafka is needed

    // Phase 4.3: Spawn reverse-check scanner (if shadow-read enabled)
    // Note: Shadow-read disabled in Phase 5 Kafka-only mode
    // TODO: Re-enable if shadow-read validation is needed

    // Phase 5: Subscribe to user online notifications
    // This allows worker to know when users come online and can process their messages
    let state_clone = state.clone();
    let _online_notification_handle = tokio::spawn(async move {
        if let Err(e) = run_user_online_notification_listener(state_clone).await {
            error!(error = %e, "User online notification listener failed");
        }
    });
    info!("Started user online notification listener (Phase 5)");
    info!("Subscribed to channel: {}", state.config.online_channel);

    // Main Kafka consumer loop
    let mut offline_message_count: u64 = 0;
    let mut last_offline_log_time = std::time::Instant::now();
    
    loop {
        match consumer.poll(std::time::Duration::from_secs(1)).await {
            Ok(Some(envelope)) => {
                match process_kafka_message(&state, &envelope).await {
                    Ok(()) => {
                        // Message processed successfully - reset offline counter
                        if offline_message_count > 0 {
                            info!(
                                offline_messages_handled = offline_message_count,
                                "Processed offline messages queue, resuming normal operation"
                            );
                            offline_message_count = 0;
                        }
                    }
                    Err(e) => {
                        let error_msg = e.to_string();
                        
                        // Phase 5: User offline is EXPECTED behavior - don't log as ERROR
                        if error_msg.contains("Recipient is offline") {
                            offline_message_count += 1;
                            
                            // Log summary every 10 seconds to avoid log spam
                            // First offline message logs immediately, then every 10 seconds
                            let now = std::time::Instant::now();
                            let should_log = offline_message_count == 1 || 
                                now.duration_since(last_offline_log_time).as_secs() >= 10;
                            
                            if should_log {
                                let salt = &state.config.logging.hash_salt;
                                if offline_message_count == 1 {
                                    debug!(
                                        recipient_hash = %log_safe_id(&envelope.recipient_id, salt),
                                        "Recipient is offline - message will be delivered when user comes online (this is normal)"
                                    );
                                } else {
                                    debug!(
                                        offline_count = offline_message_count,
                                        recipient_hash = %log_safe_id(&envelope.recipient_id, salt),
                                        "Multiple messages waiting for offline recipients (this is normal - messages will be delivered when users come online)"
                                    );
                                }
                                last_offline_log_time = now;
                            }
                        }
                        // Legacy: "No delivery queues available" is also expected
                        else if error_msg.contains("No delivery queues available") {
                            // Already logged in process_kafka_message, skip duplicate error log
                        }
                        // Real errors - log as ERROR
                        else {
                            // SECURITY: Hash recipient_id for privacy
                            let salt = &state.config.logging.hash_salt;
                            error!(
                                error = %e,
                                message_id = %envelope.message_id,
                                recipient_hash = %log_safe_id(&envelope.recipient_id, salt),
                                "Failed to process Kafka message"
                            );
                        }
                        // Do NOT commit offset on error - message will be redelivered
                        continue;
                    }
                }

                // Commit offset after successful processing
                if let Err(e) = consumer.commit() {
                    error!(
                        error = %e,
                        message_id = %envelope.message_id,
                        "Failed to commit Kafka offset after successful delivery"
                    );
                } else {
                    debug!(
                        message_id = %envelope.message_id,
                        "Kafka offset committed successfully"
                    );
                }
            }
            Ok(None) => {
                // No messages available, continue polling
                tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
            }
            Err(e) => {
                error!(error = %e, "Kafka consumer error");
                tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
            }
        }
    }
}

/// Execute a Redis operation with retry logic and auto-reconnection
async fn execute_redis_with_retry<F, T>(
    state: &WorkerState,
    operation_name: &str,
    mut operation: F,
) -> Result<T>
where
    F: FnMut(
        &mut redis::aio::MultiplexedConnection,
    ) -> std::pin::Pin<
        Box<dyn std::future::Future<Output = Result<T, redis::RedisError>> + Send + '_>,
    >,
{
    const MAX_RETRIES: u32 = 3;
    const INITIAL_BACKOFF_MS: u64 = 100;

    for attempt in 1..=MAX_RETRIES {
        let mut conn = state.redis_conn.write().await;

        match operation(&mut *conn).await {
            Ok(result) => {
                if attempt > 1 {
                    info!(
                        operation = operation_name,
                        attempt = attempt,
                        "Redis operation succeeded after retry"
                    );
                }
                return Ok(result);
            }
            Err(e) => {
                drop(conn); // Release lock before potentially reconnecting

                warn!(
                    operation = operation_name,
                    attempt = attempt,
                    max_retries = MAX_RETRIES,
                    error = %e,
                    "Redis operation failed, will retry"
                );

                if attempt < MAX_RETRIES {
                    // Exponential backoff
                    let backoff_ms = INITIAL_BACKOFF_MS * 2_u64.pow(attempt - 1);
                    tokio::time::sleep(tokio::time::Duration::from_millis(backoff_ms)).await;

                    // Try to reconnect on last retry before final attempt
                    if attempt == MAX_RETRIES - 1 {
                        info!("Attempting to reconnect to Redis...");
                        match state.redis_client.get_multiplexed_async_connection().await {
                            Ok(new_conn) => {
                                let mut conn = state.redis_conn.write().await;
                                *conn = new_conn;
                                info!("Successfully reconnected to Redis");
                            }
                            Err(reconnect_err) => {
                                error!(error = %reconnect_err, "Failed to reconnect to Redis");
                            }
                        }
                    }
                } else {
                    return Err(anyhow::anyhow!(
                        "Redis operation '{}' failed after {} retries: {}",
                        operation_name,
                        MAX_RETRIES,
                        e
                    ));
                }
            }
        }
    }

    unreachable!()
}

/// Listen for user online notifications via Redis Pub/Sub (Phase 5)
/// When a user comes online, Kafka consumer will automatically retry their messages
/// because offsets were not committed (messages remained in Kafka)
async fn run_user_online_notification_listener(state: Arc<WorkerState>) -> Result<()> {
    let client = state.redis_client.clone();
    
    loop {
        let mut pubsub_conn = match client.get_async_pubsub().await {
            Ok(conn) => conn,
            Err(e) => {
                error!(error = %e, "Failed to create Redis Pub/Sub connection, retrying in 5s...");
                tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
                continue;
            }
        };

        // Subscribe to user online notifications
        if let Err(e) = pubsub_conn.subscribe(state.config.online_channel.as_str()).await {
            error!(error = %e, channel = %state.config.online_channel, "Failed to subscribe to user online notifications, retrying in 5s...");
            tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
            continue;
        }

        info!(
            channel = %state.config.online_channel,
            "Subscribed to user online notifications (Phase 5)"
        );

        // Listen for notifications
        let mut stream = pubsub_conn.on_message();
        while let Some(msg) = stream.next().await {
            let payload: String = match msg.get_payload() {
                Ok(p) => p,
                Err(e) => {
                    error!(error = %e, "Failed to get Pub/Sub message payload");
                    continue;
                }
            };

            // Parse notification
            let notification: UserOnlineNotification = match serde_json::from_str(&payload) {
                Ok(n) => n,
                Err(e) => {
                    error!(
                        error = %e,
                        payload = %payload,
                        "Failed to parse user online notification JSON"
                    );
                    continue;
                }
            };

            // Track this user as online (for monitoring)
            {
                let mut users = state.users_online_notifications.write().await;
                users.insert(notification.user_id.clone());
                // Keep set size reasonable (cleanup old entries periodically)
                if users.len() > 10000 {
                    // Clear oldest entries (simple cleanup - in production could use LRU)
                    users.clear();
                }
            }

            // SECURITY: Hash user_id for privacy
            let salt = &state.config.logging.hash_salt;
            info!(
                user_hash = %log_safe_id(&notification.user_id, salt),
                server_instance_id = %notification.server_instance_id,
                "User came online - Kafka consumer will retry messages for this user (offsets were not committed for offline messages)"
            );

            // Note: We don't need to explicitly trigger Kafka consumer here
            // Kafka will automatically redeliver messages because offsets were not committed
            // when user was offline. The consumer will process them on next poll().
            // This notification serves for monitoring and logging purposes.
        }

        // Stream ended - reconnect
        warn!("Pub/Sub stream ended, reconnecting...");
        tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
    }
}

/// Process a single Kafka message
async fn process_kafka_message(state: &WorkerState, envelope: &KafkaMessageEnvelope) -> Result<()> {
    let message_id = &envelope.message_id;
    let recipient_id = &envelope.recipient_id;

    // SECURITY: Even in debug logs, hash recipient_id for privacy
    let salt = &state.config.logging.hash_salt;
    debug!(
        message_id = %message_id,
        recipient_hash = %log_safe_id(recipient_id, salt),
        "Processing Kafka message"
    );

    // 1. Deduplication check: have we already processed this message?
    let dedup_key = format!("{}{}", state.config.redis_key_prefixes.processed_msg, message_id);

    let exists_result: i64 = execute_redis_with_retry(state, "check_deduplication", |conn| {
        let key = dedup_key.clone();
        Box::pin(async move { redis::cmd("EXISTS").arg(&key).query_async(conn).await })
    })
    .await
    .context("Failed to check deduplication after retries")?;

    let already_processed = exists_result > 0;

    if already_processed {
        info!(
            message_id = %message_id,
            "Message already processed (deduplication)"
        );
        return Ok(()); // Return success to commit offset
    }

    // 2. –£–ú–ù–ê–Ø –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø: Check if message was already delivered directly via tx.send()
    // ========================================================================
    // –ï—Å–ª–∏ construct-server —É—Å–ø–µ—à–Ω–æ –¥–æ—Å—Ç–∞–≤–∏–ª —á–µ—Ä–µ–∑ tx.send(), –æ–Ω –ø–æ–º–µ—á–∞–µ—Ç:
    // SET delivered_direct:{message_id} 1 EX 3600
    // 
    // –ú—ã –ø—Ä–æ–≤–µ—Ä—è–µ–º —ç—Ç–æ—Ç –∫–ª—é—á –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ—Å—Ç–∞–≤–∫—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    // –≠—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–µ—Ä–≤–µ—Ä –∏ —Å–µ—Ç—å
    // ========================================================================
    let delivered_direct_key = format!("{}{}", state.config.redis_key_prefixes.delivered_direct, message_id);

    let was_delivered_direct: i64 = execute_redis_with_retry(state, "check_delivered_direct", |conn| {
        let key = delivered_direct_key.clone();
        Box::pin(async move { redis::cmd("EXISTS").arg(&key).query_async(conn).await })
    })
    .await
    .context("Failed to check delivered_direct after retries")?;

    if was_delivered_direct > 0 {
        debug!(
            message_id = %message_id,
            "Message was already delivered directly via tx.send() - skipping delivery-worker delivery"
        );
        
        // Mark as processed (deduplication) and commit offset
        let _: String = execute_redis_with_retry(state, "mark_skipped_message_processed", |conn| {
            let key = dedup_key.clone();
            let ttl = state.config.message_ttl_days * construct_server::config::SECONDS_PER_DAY;
            Box::pin(async move {
                redis::cmd("SETEX")
                    .arg(&key)
                    .arg(ttl)
                    .arg("1")
                    .query_async(conn)
                    .await
            })
        })
        .await
        .context("Failed to mark skipped message as processed")?;
        
        return Ok(()); // Success - commit Kafka offset
    }

    // 3. Check if recipient is online by looking up their server instance
    // Look up user:{user_id}:server_instance_id to find which server handles this user
    let user_server_key = format!("{}{}:server_instance_id", state.config.redis_key_prefixes.user, recipient_id);
    
    let server_instance_id: Option<String> = execute_redis_with_retry(state, "check_user_online", |conn| {
        let key = user_server_key.clone();
        Box::pin(async move {
            redis::cmd("GET")
                .arg(&key)
                .query_async(conn)
                .await
        })
    })
    .await
    .context("Failed to check user online status after retries")?;

    // ========================================================================
    // SIMPLIFIED ARCHITECTURE: Redis Lists instead of Pub/Sub
    // ========================================================================
    //
    // –ü–†–û–ë–õ–ï–ú–ê Pub/Sub: fire-and-forget - –µ—Å–ª–∏ –Ω–∏–∫—Ç–æ –Ω–µ —Å–ª—É—à–∞–µ—Ç, —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—è–Ω–æ!
    // –†–ï–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis Lists (RPUSH) - –Ω–∞–¥—ë–∂–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –¥–æ—Å—Ç–∞–≤–∫–∏
    //
    // –¢–µ–ø–µ—Ä—å delivery-worker –í–°–ï–ì–î–ê —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Redis List:
    // - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ–Ω–ª–∞–π–Ω: delivery_queue:{server_instance_id}
    // - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ñ—Ñ–ª–∞–π–Ω: delivery_queue:offline:{recipient_id}
    //
    // construct-server –æ–ø—Ä–∞—à–∏–≤–∞–µ—Ç delivery_queue:{server_instance_id} —á–µ—Ä–µ–∑ poll_delivery_queue
    // –∏ –¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞–º —á–µ—Ä–µ–∑ tx.send()
    //
    // –ì–∞—Ä–∞–Ω—Ç–∏–∏:
    // - –°–æ–æ–±—â–µ–Ω–∏—è –ù–ï —Ç–µ—Ä—è—é—Ç—Å—è (Redis List persistent)
    // - Periodic polling –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –¥–æ—Å—Ç–∞–≤–∫—É –¥–∞–∂–µ –±–µ–∑ Pub/Sub
    // - construct-server –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–¥–æ–∂–¥—É—Ç
    // ========================================================================

    use construct_server::config::SECONDS_PER_DAY;
    let ttl_seconds = state.config.message_ttl_days * SECONDS_PER_DAY;
    
    // Serialize envelope to MessagePack
    let message_bytes = rmp_serde::encode::to_vec_named(&envelope)
        .context("Failed to serialize Kafka envelope to MessagePack")?;

    if let Some(server_instance_id) = server_instance_id {
        // User is online on this server instance - push to delivery_queue:{server_instance_id}
        // Server's delivery listener will poll this queue and deliver via WebSocket
        let delivery_queue_key = format!("{}{}", state.config.delivery_queue_prefix, server_instance_id);
        
        let _: i64 = execute_redis_with_retry(state, "push_to_delivery_queue", |conn| {
            let key = delivery_queue_key.clone();
            let msg = message_bytes.clone();
            let ttl = ttl_seconds;
            Box::pin(async move {
                // RPUSH to maintain FIFO order
                let _: () = redis::cmd("RPUSH").arg(&key).arg(&msg).query_async(conn).await?;
                // Refresh TTL on the key
                let _: () = redis::cmd("EXPIRE").arg(&key).arg(ttl).query_async(conn).await?;
                Ok::<i64, redis::RedisError>(1)
            })
        })
        .await
        .context("Failed to push message to delivery queue after retries")?;
        
        info!(
            message_id = %message_id,
            recipient_hash = %log_safe_id(recipient_id, &state.config.logging.hash_salt),
            queue_key = %delivery_queue_key,
            "üì§ Message pushed to delivery_queue (recipient online) - server will poll and deliver"
        );
        
        // Mark message as processed
        let _: String = execute_redis_with_retry(state, "mark_message_processed", |conn| {
            let key = dedup_key.clone();
            let ttl = ttl_seconds;
            Box::pin(async move {
                redis::cmd("SETEX")
                    .arg(&key)
                    .arg(ttl)
                    .arg("1")
                    .query_async(conn)
                    .await
            })
        })
        .await
        .context("Failed to mark message as processed after retries")?;
        
        return Ok(()); // Success - Kafka offset will be committed
    } else {
        // CRITICAL FIX: User is offline - save message to Redis delivery_queue for later delivery
        // Problem: Message was already read from Kafka via recv(), but offset not committed.
        // If we do continue, next poll() will return the NEXT message, and current message is LOST.
        // Solution: Save message to Redis delivery_queue with recipient_id as key, so it can be
        // delivered when user comes online via the delivery listener mechanism.
        
        // Get all active server instances to find where to queue the message
        // We'll queue it to a special key that will be processed when user comes online
        // Format: delivery_queue:offline:{recipient_id} -> list of messages
        let offline_queue_key = format!("{}offline:{}", state.config.delivery_queue_prefix, recipient_id);
        
        // Serialize envelope to MessagePack for Redis storage
        let message_bytes = rmp_serde::encode::to_vec_named(envelope)
            .context("Failed to serialize Kafka envelope to MessagePack for offline queue")?;
        
        // Save to Redis list with TTL (will be cleaned up after delivery or TTL expiry)
        use construct_server::config::SECONDS_PER_DAY;
        let ttl_seconds = state.config.message_ttl_days * SECONDS_PER_DAY;
        
        let _: i64 = execute_redis_with_retry(state, "save_offline_message", |conn| {
            let key = offline_queue_key.clone();
            let msg = message_bytes.clone();
            let ttl = ttl_seconds;
            Box::pin(async move {
                // Use RPUSH to add message to list (FIFO order - oldest first)
                // This maintains order when messages are later moved to delivery_queue
                let _: () = redis::cmd("RPUSH").arg(&key).arg(&msg).query_async(conn).await?;
                // Set TTL on the key (messages will expire if not delivered)
                let _: () = redis::cmd("EXPIRE").arg(&key).arg(ttl).query_async(conn).await?;
                Ok::<i64, redis::RedisError>(1)
            })
        })
        .await
        .context("Failed to save offline message to Redis after retries")?;
        
        // Mark message as processed to prevent duplicate delivery when user comes online
        // (the message is saved in Redis, so it will be delivered via delivery listener)
        let _: String = execute_redis_with_retry(state, "mark_message_processed_for_offline", |conn| {
            let key = dedup_key.clone();
            let ttl = ttl_seconds;
            Box::pin(async move {
                redis::cmd("SETEX")
                    .arg(&key)
                    .arg(ttl)
                    .arg("1")
                    .query_async(conn)
                    .await
            })
        })
        .await
        .context("Failed to mark offline message as processed after retries")?;
        
        info!(
            message_id = %message_id,
            recipient_hash = %log_safe_id(recipient_id, &state.config.logging.hash_salt),
            queue_key = %offline_queue_key,
            "Recipient is offline - message saved to Redis delivery_queue for later delivery"
        );
        
        // Return success to commit offset - message is safely stored in Redis
        // When user comes online, delivery listener will process messages from delivery_queue
        return Ok(());
    }
}
